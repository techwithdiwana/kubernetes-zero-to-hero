# Kubernetes Zero to Hero â€” Day 11: Minikube + Helm + RBAC + HPA

<p align="center">
  <img src="https://img.shields.io/badge/Kubernetes-1.29-blue?logo=kubernetes&logoColor=white" />
  <img src="https://img.shields.io/badge/Minikube-Local%20Cluster-green?logo=kubernetes" />
  <img src="https://img.shields.io/badge/Helm-Chart%20Deployed-blue?logo=helm" />
  <img src="https://img.shields.io/badge/Autoscaling-HPA-orange?logo=kubernetes" />
  <img src="https://img.shields.io/badge/License-MIT-yellow" />
</p>

## ğŸ“Œ Overview

This repo is **Day 11** of the *Kubernetes Zero to Hero* series by **Tech With Diwana**.

You will build a small **production-style setup** on a local Minikube cluster:

- Deploy a frontend app using **Helm + Ingress**
- Configure **RBAC** (ServiceAccount, Role, RoleBinding)
- Deploy a backend service with **CPU/Memory requests & limits**
- Configure **Horizontal Pod Autoscaler (HPA)** based on CPU
- Use a **load generator Job** to trigger real autoscaling
- Get a **practical view of VPA** (Vertical Pod Autoscaler) usage

This README is written so **anyone can follow stepâ€‘byâ€‘step on their own laptop**.

> â„¹ï¸ VPA controllers are **not installed** in this demo because upstream manifests change often
> and VPA is rarely used in production compared to HPA. We keep a simple `vpa.yaml` as reference only.

---

## ğŸ§° Prerequisites

You need:

- [Minikube](https://minikube.sigs.k8s.io/docs/start/)
- `kubectl`
- Helm 3 (on Windows you can use: `choco install kubernetes-helm`)
- Docker (or another Minikube-supported driver)

### Start Minikube & enable addons

```bash
minikube start
minikube addons enable ingress
minikube addons enable metrics-server
```

**Why these addons?**

- **Ingress** â†’ provides NGINX Ingress Controller so we can use `http://twd.local`.
- **metrics-server** â†’ exposes CPU/Memory metrics used by **HPA** and `kubectl top`.

Create a namespace for all Dayâ€‘11 objects:

```bash
kubectl create namespace twd-apps
```

---

## ğŸ§± Architecture

```text
Frontend (Helm)
  â””â”€ Ingress (twd.local)

Backend: resource-demo (Deployment + Service)
  â””â”€ HPA (CPU based)
      â””â”€ Load Generator Job (busybox â†’ HTTP calls)

RBAC: read-pod-sa + Role + RoleBinding
```

---

## ğŸ“ Folder Structure

```text
day11-k8s-zero-to-hero/
â”œâ”€ frontend-app/                  # Helm chart for frontend
â”œâ”€ k8s-manifests/
â”‚  â”œâ”€ rbac.yaml                   # ServiceAccount + Role + RoleBinding
â”‚  â”œâ”€ app-resources.yaml          # resource-demo Deployment
â”‚  â”œâ”€ service-resource-demo.yaml  # ClusterIP Service for resource-demo
â”‚  â”œâ”€ hpa.yaml                    # Horizontal Pod Autoscaler
â”‚  â”œâ”€ load-generator.yaml         # Busybox-based load generator Job
â”‚  â””â”€ vpa.yaml                    # VPA object (reference only)
â””â”€ README.md
```

To use this repo:

```bash
git clone <your-fork-url>
cd day11-k8s-zero-to-hero
```

---

# ğŸš€ Stepâ€‘byâ€‘Step

Each step explains **why**, **how**, and **what you achieve**.

---

## 1ï¸âƒ£ Deploy Frontend with Helm + Ingress

### Why?

Helm is the *package manager* for Kubernetes. Most real clusters use Helm charts to manage apps.
Here we use a simple chart called `frontend-app` that serves a frontend image and exposes it via Ingress.

The chart is already in `frontend-app/`. Key values are in `values.yaml`:

```yaml
image:
  repository: techwithdiwana/frontend
  tag: "v1"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: "nginx"
  hosts:
    - host: twd.local
      paths:
        - path: /
          pathType: Prefix
```

### Deploy

```bash
helm upgrade --install twd-frontend ./frontend-app   --namespace twd-apps --create-namespace
```

Check resources:

```bash
kubectl get pods -n twd-apps
kubectl get svc -n twd-apps
kubectl get ingress -n twd-apps
```

### Access via browser

Get Minikube IP:

```bash
minikube ip
```

Add this line in your hosts file (Windows: `C:\Windows\System32\drivers\etc\hosts`):

```text
<MINIKUBE_IP>  twd.local
```

Now open: **http://twd.local**

âœ… **Result:** a running frontend app managed by Helm, exposed through Ingress with a friendly hostname.

---

## 2ï¸âƒ£ RBAC: ServiceAccount + Role + RoleBinding

### Why?

By default, Pods should **not** run with clusterâ€‘admin rights. RBAC lets us give each workload
only the permissions it needs. Here we create a ServiceAccount that can **only read pods**
in the `twd-apps` namespace.

### Apply RBAC manifest

```bash
kubectl apply -f k8s-manifests/rbac.yaml
```

This creates:

- `read-pod-sa` â€“ ServiceAccount (identity for workloads)
- `pod-reader-role` â€“ Role allowing `get`/`list` on pods
- `pod-reader-binding` â€“ RoleBinding linking the SA to that Role

### Verify & test

```bash
kubectl get sa -n twd-apps
kubectl get role -n twd-apps
kubectl get rolebinding -n twd-apps
```

RBAC test:

```bash
kubectl auth can-i list pods   --as=system:serviceaccount:twd-apps:read-pod-sa   -n twd-apps
```

Expected: `yes`

âœ… **Result:** a leastâ€‘privilege identity exactly like you would use in production.

---

## 3ï¸âƒ£ Backend: `resource-demo` Deployment + Service

### Why?

We need a backend workload with **CPU/memory requests & limits** so that HPA can calculate
utilization percentages correctly.

### Apply Deployment & Service

```bash
kubectl apply -f k8s-manifests/app-resources.yaml
kubectl apply -f k8s-manifests/service-resource-demo.yaml
```

Check:

```bash
kubectl get deploy resource-demo -n twd-apps
kubectl get pods -l app=resource-demo -n twd-apps
kubectl get svc resource-demo -n twd-apps
kubectl top pods -n twd-apps
```

âœ… **Result:** a backend deployment (`resource-demo`) reachable via a ClusterIP service
inside the cluster and reporting metrics via metricsâ€‘server.

---

## 4ï¸âƒ£ Horizontal Pod Autoscaler (HPA)

### Why?

HPA automatically adjusts the **number of Pods** based on metrics. In this lab we scale
`resource-demo` based on CPU utilization.

### Apply HPA

```bash
kubectl apply -f k8s-manifests/hpa.yaml
kubectl get hpa -n twd-apps
```

HPA configuration (from `hpa.yaml`):

- `scaleTargetRef` â†’ `resource-demo` deployment
- `averageUtilization: 50` â†’ target 50% of requested CPU
- `minReplicas: 1`, `maxReplicas: 5` â†’ scaling range

âœ… **Result:** a scaling policy ready to react when CPU usage increases.

---

## 5ï¸âƒ£ Load Generator Job (Real CPU Load)

### Why a Job?

HPA does nothing until there is **real CPU pressure**. The `load-generator` Job simulates
traffic by repeatedly calling the `resource-demo` Service for ~3 minutes.

The Job uses a **busyboxâ€‘friendly shell loop** (no `SECONDS` variable) so it runs correctly
in all environments.

### Start the load

```bash
kubectl apply -f k8s-manifests/load-generator.yaml -n twd-apps
```

Watch HPA in one terminal:

```bash
kubectl get hpa -n twd-apps -w
```

Watch pods and metrics in another terminal:

```bash
kubectl get pods -n twd-apps
kubectl top pods -n twd-apps
```

You should see:

- HPA `TARGETS` CPU column exceeding `50%`
- Replica count increasing from `1` â†’ `2` or `3`

The Job finishes automatically after about 180 seconds. You can also delete it manually:

```bash
kubectl delete -f k8s-manifests/load-generator.yaml -n twd-apps
```

âœ… **Result:** you have observed **real autoscaling** driven by CPU metrics.

---

## 6ï¸âƒ£ VPA â€“ Honest Production Perspective

Vertical Pod Autoscaler (VPA) adjusts **CPU/memory requests** of Pods instead of changing
replica count. It is powerful but also more disruptive because it may evict Pods to apply
new resource values.

For this repo we keep things practical:

> **In real production, most companies prefer HPA over VPA.  
> VPA is NOT widely used because it evicts pods to update resources, causing risk of downtime and conflicts with HPA.**

The file `k8s-manifests/vpa.yaml` is provided only as a reference object targeting
`resource-demo` for learners who want to experiment in a cluster where VPA controllers
are already installed.

---

## 7ï¸âƒ£ Verification Checklist

Run:

```bash
kubectl get pods -n twd-apps
kubectl get deploy -n twd-apps
kubectl get svc -n twd-apps
kubectl get ingress -n twd-apps
kubectl get hpa -n twd-apps
kubectl top pods -n twd-apps
```

You should see:

- Frontend deployment via Helm and Ingress.
- Backend `resource-demo` deployment & service.
- HPA with current/desired replicas.
- Pod metrics via `kubectl top`.

---

## 8ï¸âƒ£ Cleanup

```bash
helm uninstall twd-frontend -n twd-apps || true
kubectl delete -f k8s-manifests/hpa.yaml -n twd-apps || true
kubectl delete -f k8s-manifests/load-generator.yaml -n twd-apps || true
kubectl delete -f k8s-manifests/service-resource-demo.yaml -n twd-apps || true
kubectl delete -f k8s-manifests/app-resources.yaml -n twd-apps || true
kubectl delete -f k8s-manifests/rbac.yaml -n twd-apps || true
kubectl delete namespace twd-apps || true
```

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## â­ Author

**Tech With Diwana** â€” Kubernetes Zero to Hero Series
