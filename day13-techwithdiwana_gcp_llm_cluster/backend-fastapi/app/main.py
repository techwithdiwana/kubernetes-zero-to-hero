from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

try:
    from sentence_transformers import SentenceTransformer, util
except ImportError:
    SentenceTransformer = None
    util = None

app = FastAPI(
    title="TechWithDiwana LLM Vector Service",
    description="Local semantic search + FAQ style responder. No paid external APIs.",
    version="2.0.0",
)


class ChatRequest(BaseModel):
    message: str


class ChatResponse(BaseModel):
    reply: str
    similar_docs: List[str]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Knowledge base: short English facts for your demo topics
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KB_DOCS = [
    # 0 â€“ Multi-master basics
    "In a Kubernetes multi-master setup, you run multiple control-plane nodes behind a load balancer exposing the kube-apiserver on a single stable address.",
    # 1 â€“ Why HA / benefits
    "High availability for the Kubernetes control plane means that if one master node fails, clients can still talk to a healthy API server and workloads keep running.",
    # 2 â€“ Master failure behavior
    "When one master node fails but etcd still has quorum, the remaining control-plane nodes continue serving the API and the cluster stays online.",
    # 3 â€“ Load balancer role
    "The load balancer in front of the Kubernetes API servers spreads kubectl traffic across healthy masters and hides individual node IPs behind one stable endpoint.",
    # 4 â€“ kubeconfig usage
    "A kubeconfig file stores cluster endpoints and credentials so that kubectl on your laptop can talk securely to the Kubernetes API, usually via the load balancer DNS.",
    # 5 â€“ Why use LB DNS instead of node IP
    "You normally point kubeconfig at the load balancer DNS instead of a single master IP, so that you do not care which control-plane node is currently serving the request.",
    # 6 â€“ Ingress basics
    "Kubernetes Ingress lets you route external HTTP and HTTPS traffic to services using hostnames and paths, instead of exposing every service as a NodePort or LoadBalancer.",
    # 7 â€“ Ingress advantages
    "Compared to NodePort or port-forwarding, an Ingress controller centralises routing, supports clean domain names and TLS termination, and reduces the number of public load balancers.",
    # 8 â€“ Host-based routing
    "With host-based routing, an Ingress rule can send traffic for different domains, such as api.example.com and app.example.com, to different backend services inside the cluster.",
    # 9 â€“ Path-based routing
    "With path-based routing, a single hostname can route /api, /frontend or /metrics to different services, which is ideal for microservice and gateway setups.",
    # 10 â€“ SSL / cert-manager
    "cert-manager automates issuing and renewing TLS certificates inside Kubernetes by talking to certificate authorities such as Letâ€™s Encrypt.",
    # 11 â€“ Letâ€™s Encrypt flow
    "For HTTP-01 challenges, cert-manager creates temporary Ingress rules so that Letâ€™s Encrypt can verify the domain and then issue a free certificate.",
    # 12 â€“ ClusterIssuer meaning
    "A ClusterIssuer is a cluster-wide cert-manager resource that defines how certificates should be requested, for example using Letâ€™s Encrypt production servers.",
    # 13 â€“ Vector search concept
    "Vector embeddings represent text as numeric vectors so that you can use cosine similarity to find semantically similar sentences instead of simple keyword matches.",
    # 14 â€“ Why no paid LLM API
    "This demo does not call any paid LLM API; all answers are generated by a small local embedding model that only searches a fixed knowledge base.",
    # 15 â€“ Project architecture overview
    "This Tech With Diwana demo runs a FastAPI backend for vector search, a Node.js gateway for HTTP APIs, and an Nginx frontend serving the chat UI, all inside Kubernetes.",
    # 16 â€“ Node.js gateway role
    "The Node.js gateway exposes a simple /api/chat endpoint, forwards requests to the FastAPI service, and hides internal service names from the browser.",
    # 17 â€“ FastAPI backend role
    "The FastAPI backend encodes the user question, compares it with precomputed embeddings of the knowledge base, and returns the best matching explanation.",
    # 18 â€“ Frontend role
    "The frontend is a static Nginx site that provides a chat-style UI and sends JSON requests to the Node.js gateway through the Kubernetes Ingress.",
    # 19 â€“ GCP cost hygiene
    "On Google Cloud you should delete unused VM instances, disks, external IP addresses and load balancers after your demo so that you do not burn through free credits.",
]


_model = None
_kb_embeddings = None


def get_model():
    """Lazy-load the sentence-transformer model."""
    global _model
    if _model is None:
        if SentenceTransformer is None:
            raise RuntimeError(
                "sentence-transformers is not installed. "
                "Install it with: pip install sentence-transformers"
            )
        _model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    return _model


def get_kb_embeddings():
    """Precompute embeddings for all KB documents."""
    global _kb_embeddings
    if _kb_embeddings is None:
        model = get_model()
        _kb_embeddings = model.encode(KB_DOCS, convert_to_tensor=True)
    return _kb_embeddings


@app.get("/healthz")
async def healthz():
    return {"status": "ok", "service": "techwithdiwana-llm-vector"}


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint.

    - Uses local embeddings to find the most relevant KB_DOCS entry.
    - Applies a similarity threshold: if the question is unrelated,
      returns a polite fallback message.
    """
    message = request.message.strip()

    if not message:
        return ChatResponse(
            reply=(
                "Hi from Tech With Diwana! ðŸ‘‹\n\n"
                "Ask me about:\n"
                "- Kubernetes multi-master and high availability\n"
                "- Load balancers for the API server\n"
                "- kubeconfig and connecting from your laptop\n"
                "- Ingress, SSL and cert-manager\n"
                "- How this demo architecture works on GCP"
            ),
            similar_docs=[],
        )

    model = get_model()
    kb_embeddings = get_kb_embeddings()

    # Encode user query and compute cosine similarity to all docs
    query_emb = model.encode(message, convert_to_tensor=True)
    scores = util.cos_sim(query_emb, kb_embeddings)[0]  # type: ignore

    # Get top-k matches
    top_k = min(3, len(KB_DOCS))
    topk = scores.topk(top_k)  # type: ignore
    best_scores = topk.values.tolist()
    best_indices = topk.indices.tolist()

    best_score = float(best_scores[0])
    best_doc = KB_DOCS[best_indices[0]]
    similar = [KB_DOCS[i] for i in best_indices]

    # If similarity is too low, question is out of scope
    THRESHOLD = 0.25
    if best_score < THRESHOLD:
        fallback = (
            "TechWithDiwana Demo Bot:\n\n"
            "I only know about this specific demo:\n"
            "- Kubernetes HA and multi-master on GCP\n"
            "- API server load balancer and kubeconfig\n"
            "- Ingress, SSL, cert-manager and DNS\n"
            "- The FastAPI + Node.js + Nginx architecture\n\n"
            "Please ask about one of those topics ðŸ™‚"
        )
        return ChatResponse(reply=fallback, similar_docs=[])

    # Normal case: answer using the best-matching document
    reply_lines = [
        "ðŸ¤– TechWithDiwana Demo Bot",
        "",
        "Here is an answer based on the local knowledge base (no paid API calls):",
        "",
        best_doc,
        "",
    ]

    # Add a few related bullets so reply looks richer
    if len(similar) > 1:
        reply_lines.append("Related points:")
        for extra in similar[1:]:
            if extra != best_doc:
                reply_lines.append(f"- {extra}")

    return ChatResponse(reply="\n".join(reply_lines), similar_docs=similar)
